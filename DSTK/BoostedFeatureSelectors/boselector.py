from __future__ import division

from sklearn.linear_model import LogisticRegressionCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from base_selector import BaseSelector


class Bolasso(BaseSelector):
    """Bolasso feature selection technique, based on the article

    `F. R. Bach, Bolasso: model consistent Lasso estimation through the bootstrap, ICML '08`

    This feature selection wrapper trains a `num_bootstrap` sklearn LogisticRegressionCV classifiers with L1-penalty
    on a bootstrapped subset of the data with size `bootstrap_fraction`. It will store an internal Dataframe with the
    raw coefficients of the trained classifiers and exposes a method to get some summary statistics of the full Bolasso
    DF.

    Parameters
    ----------
    bootstrap_fraction: Fraction of the data that will be bootstrap sampled with replacement.

    random_seed: Fix the seed for the bootstrap generator

    kwargs: All the arguments that the sklearn LogisticRegressionCV classifier takes.

    Attributes
    ----------
    logit: The initialized LogisticRegressionCV classifier

    coeff_df: The internal DataFrame holding all the individual coefficients

    """

    def __init__(self, bootstrap_fraction, random_seed=None, **kwargs):
		self.cv = kwargs.get('cv', None)
		
        # The following parameters are changed from default
        # since we want to induce sparsity in the final
        # feature set of Bolasso.
        # liblinear is needed to be working with 'L1' penalty.
        if self.cv == False:
		self.C = kwargs.get('C', 10)
		self.fit_intercept = kwargs.get('fit_intercept', True)
		self.dual = kwargs.get('dual', False)
		self.tol = kwargs.get('tol', 1e-4)
		self.max_iter = kwargs.get('max_iter', 100)
		self.class_weight = kwargs.get('class_weight', None)
		self.n_jobs = kwargs.get('n_jobs', 1)
		self.verbose = kwargs.get('verbose', 0)
		self.intercept_scaling = kwargs.get('intercept_scaling', 1.0)
		self.multi_class = kwargs.get('multi_class', 'ovr')
		self.random_state = kwargs.get('random_state', None)        
			
		self.logit = LogisticRegression(
			C=self.C,
			fit_intercept=self.fit_intercept,
			dual=self.dual,
			penalty='l1',
			scoring=self.scoring,
			solver='liblinear',
			tol=self.tol,
			max_iter=self.max_iter,
			class_weight=self.class_weight,
			n_jobs=self.n_jobs,
			verbose=self.verbose,
			refit=self.refit,
			intercept_scaling=self.intercept_scaling,
			multi_class=self.multi_class,
			random_state=self.random_state
		)
        else:
		self.Cs = kwargs.get('Cs', 10)
		self.fit_intercept = kwargs.get('fit_intercept', True)
		self.dual = kwargs.get('dual', False)
		self.scoring = kwargs.get('scoring', None)
		self.tol = kwargs.get('tol', 1e-4)
		self.max_iter = kwargs.get('max_iter', 100)
		self.class_weight = kwargs.get('class_weight', None)
		self.n_jobs = kwargs.get('n_jobs', 1)
		self.verbose = kwargs.get('verbose', 0)
		self.refit = kwargs.get('refit', True)
		self.intercept_scaling = kwargs.get('intercept_scaling', 1.0)
		self.multi_class = kwargs.get('multi_class', 'ovr')
		self.random_state = kwargs.get('random_state', None)
			
		self.logit = LogisticRegressionCV(
			Cs=self.Cs,
			fit_intercept=self.fit_intercept,
			cv=self.cv,
			dual=self.dual,
			penalty='l1',
			scoring=self.scoring,
			solver='liblinear',
			tol=self.tol,
			max_iter=self.max_iter,
			class_weight=self.class_weight,
			n_jobs=self.n_jobs,
			verbose=self.verbose,
			refit=self.refit,
			intercept_scaling=self.intercept_scaling,
			multi_class=self.multi_class,
			random_state=self.random_state
		)
        
        super(Bolasso, self).__init__(bootstrap_fraction, self.logit, random_seed=random_seed)

    def _get_feature_coeff(self):
        return self.logit.coef_.flatten().tolist()


class Botree(BaseSelector):
    """Botree feature selection technique, based on the article

       `F. R. Bach, Bolasso: model consistent Lasso estimation through the bootstrap, ICML '08`

       This feature selection wrapper trains a `num_bootstrap` sklearn DecisionTree classifiers
       on a bootstrapped subset of the data with size `bootstrap_fraction`. It will store an internal Dataframe with the
       raw coefficients of the trained classifiers and exposes a method to get some summary statistics of the full Botree
       DF.

       Parameters
       ----------
       bootstrap_fraction: Fraction of the data that will be bootstrap sampled with replacement.

       random_seed: Fix the seed for the bootstrap generator

       kwargs: All the arguments that the sklearn DecisionTreeClassifier takes.

       Attributes
       ----------
       dtc: The initialized DecisionTree classifier

       coeff: The internal DataFrame holding all the individual coefficients

       """

    def __init__(self, bootstrap_fraction, random_seed=None, feature_importance_metric=None, feature_importance_threshold=None, **kwargs):

        self.criterion = kwargs.get('criterion', "gini")
        self.splitter = kwargs.get('splitter', "best")
        self.max_depth = kwargs.get('max_depth', None)
        self.min_samples_split = kwargs.get('min_samples_split', 2)
        self.min_samples_leaf = kwargs.get('min_samples_leaf', 1)
        self.min_weight_fraction_leaf = kwargs.get('min_weight_fraction_leaf', 0.)
        self.max_features = kwargs.get('max_features', None)
        self.max_leaf_nodes = kwargs.get('random_state', None)
        self.class_weight = kwargs.get('max_leaf_nodes', None)
        self.random_state = kwargs.get('class_weight', None)
        self.presort = kwargs.get('presort', False)

        self.dtc = DecisionTreeClassifier(
            criterion=self.criterion,
            splitter=self.splitter,
            max_depth=self.max_depth,
            min_samples_split=self.min_samples_split,
            min_samples_leaf=self.min_samples_leaf,
            min_weight_fraction_leaf=self.min_weight_fraction_leaf,
            max_features=self.max_features,
            max_leaf_nodes=self.max_leaf_nodes,
            class_weight=self.class_weight,
            random_state=self.random_state,
            presort=self.presort
        )

        super(Botree, self).__init__(bootstrap_fraction, self.dtc, random_seed=random_seed, feature_importance_metric=feature_importance_metric, feature_importance_threshold=feature_importance_threshold)

    def _get_feature_coeff(self):
        return self.dtc.feature_importances_.flatten().tolist()


class Boforest(BaseSelector):
    """Boforest feature selection technique, based on the article

       `F. R. Bach, Bolasso: model consistent Lasso estimation through the bootstrap, ICML '08`

       This feature selection wrapper trains `num_bootstrap` sklearn RandomForest classifiers
       on a bootstrapped subset of the data with size `bootstrap_fraction`. It will store an internal Dataframe with the
       raw coefficients of the trained classifiers and exposes a method to get some summary statistics of the full Botree
       DF.

       Parameters
       ----------
       bootstrap_fraction: Fraction of the data that will be bootstrap sampled with replacement.

       random_seed: Fix the seed for the bootstrap generator

       kwargs: All the arguments that the sklearn RandomForest classifier takes.

       Attributes
       ----------
       rfc: The initialized DecisionTree classifier

       coeff_df: The internal DataFrame holding all the individual coefficients

       """

    def __init__(self, bootstrap_fraction, random_seed=None, **kwargs):
        self.n_estimators = kwargs.get('n_estimators', 10)
        self.criterion = kwargs.get('criterion', "gini")
        self.max_depth = kwargs.get('max_depth', None)
        self.min_samples_split = kwargs.get('min_samples_split', 2)
        self.min_samples_leaf = kwargs.get('min_samples_leaf', 1)
        self.min_weight_fraction_leaf = kwargs.get('min_weight_fraction_leaf', 0.)
        self.max_features = kwargs.get('max_features', "auto")
        self.max_leaf_nodes = kwargs.get('max_leaf_nodes', None)
        self.bootstrap = kwargs.get('bootstrap', True)
        self.oob_score = kwargs.get('oob_score', False)
        self.n_jobs = kwargs.get('n_jobs', 1)
        self.random_state = kwargs.get('random_state', None)
        self.verbose = kwargs.get('verbose', 0)
        self.warm_start = kwargs.get('warm_start', False)
        self.class_weight = kwargs.get('class_weight', None)

        self.rfc = RandomForestClassifier(
            self.n_estimators,
            self.criterion,
            self.max_depth,
            self.min_samples_split,
            self.min_samples_leaf,
            self.min_weight_fraction_leaf,
            self.max_features,
            self.max_leaf_nodes,
            self.bootstrap,
            self.oob_score,
            self.n_jobs,
            self.random_state,
            self.verbose,
            self.warm_start,
            self.class_weight
        )

        super(Boforest, self).__init__(bootstrap_fraction, self.rfc, random_seed=random_seed)

    def _get_feature_coeff(self):
        return self.rfc.feature_importances_.flatten().tolist()
